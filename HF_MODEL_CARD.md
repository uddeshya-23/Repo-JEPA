---
language: en
tags:
- code
- semantic-search
- jepa
- code-search
- self-supervised
license: mit
datasets:
- claudios/code_search_net
metrics:
- mrr
base_model: microsoft/codebert-base
---

# Repo-JEPA: SOTA Semantic Code Search (90.5% MRR)

<div align="center">

[![GitHub](https://img.shields.io/badge/GitHub-Repo-blue)](https://github.com/uddeshya-23/Repo-JEPA)
[![Paper](https://img.shields.io/badge/Research-Attribution-green)](https://github.com/uddeshya-23/Repo-JEPA/blob/main/RESEARCH_ATTRIBUTION.md)
[![Demo](https://img.shields.io/badge/Try-Interactive_CLI-orange)](https://github.com/uddeshya-23/Repo-JEPA#quick-start)

</div>

## üéØ What Makes This Different?

Traditional code search looks for **keywords**. Repo-JEPA understands **intent**.

**Example:**
```python
# Your codebase:
def authenticate_user(credentials):
    """Verify user identity and generate session token."""
    ...

# Traditional search:
"login handler" ‚Üí ‚ùå No results

# Repo-JEPA:
"where do we validate user credentials?" ‚Üí ‚úÖ authenticate_user() (Score: 0.89)
```

---

## üèÜ Benchmark Results

Evaluated on **1,000 unseen CodeSearchNet Python functions**:

| Metric | Result | Industry Baseline |
|--------|--------|------------------|
| **MRR** | **0.9052** | 0.60 |
| **Hits@1** | **86.2%** | ~55% |
| **Hits@5** | **95.9%** | ~75% |
| **Hits@10** | **97.3%** | ~85% |
| **Median Rank** | **1.0** | ~3.0 |

**Translation**: The model returns the *exact* correct function as the #1 result **86% of the time**.

---

## üöÄ Quick Start

### Installation
```bash
pip install transformers torch
```

### Usage (3 Lines of Code)

```python
from transformers import AutoModel, AutoTokenizer

# 1. Load model
model = AutoModel.from_pretrained("uddeshya-k/RepoJepa", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")

# 2. Encode your code
code = "def calculate_discount(price, coupon): return price * (1 - coupon)"
code_embed = model.encode_code(**tokenizer(code, return_tensors="pt"))

# 3. Search with natural language
query = "how to apply discount codes?"
query_embed = model.encode_query(**tokenizer(query, return_tensors="pt"))

# Similarity score (higher = better match)
from torch.nn.functional import cosine_similarity
score = cosine_similarity(code_embed, query_embed)
print(f"Match: {score.item():.4f}")  # 0.89
```

### Production Usage (Automated Indexing)

For real projects, use the built-in indexer:

```bash
#Clone repo
git clone https://github.com/uddeshya-23/Repo-JEPA
cd Repo-JEPA
pip install -r requirements.txt

# Index your entire project
python src/utils/indexer.py --path /your/project --save project.index

# Search interactively
python cli_search.py
```

---

## üß† Technical Architecture

### Core Innovation: JEPA + VICReg

**JEPA (Joint Embedding Predictive Architecture)**  
Pioneered by [Yann LeCun](https://twitter.com/ylecun) at Meta AI, JEPA learns to align code and natural language in a shared embedding space without requiring labeled pairs.

**VICReg Loss** (Bardes et al., ICLR 2022)  
Prevents "representation collapse" where different functions would get identical embeddings:
- **Variance**: Each function has a unique fingerprint
- **Invariance**: Semantically similar code clusters together
- **Covariance**: Decorrelates embedding dimensions

### Architecture Diagram

```
Query: "how to validate emails?"
    ‚Üì
Target Encoder (EMA-updated)
    ‚Üì
Query Embedding (768-dim)
    ‚Üì
Cosine Similarity ‚Üê Code Embedding (768-dim)
                        ‚Üë
                   Context Encoder
                        ‚Üë
Code: def verify_email(addr): regex.match(...)
```

### Training Details
- **Base Model**: CodeBERT (RoBERTa-style, 110M params)
- **Dataset**: 411K Python functions from CodeSearchNet
- **Hardware**: NVIDIA H100 PCIe (80GB VRAM)
- **Training Time**: ~1.5 hours (10 epochs)
- **Optimizer**: AdamW + OneCycleLR
- **Framework**: PyTorch 2.6.0, Transformers 4.x

---

## üìä Comparison with Existing Methods

| Approach | Privacy | Semantic | Accuracy (MRR) | Speed |
|----------|---------|----------|----------------|-------|
| **grep/ripgrep** | ‚úÖ Local | ‚ùå None | ~0.15 | ‚ö° Instant |
| **IDE Keyword** | ‚úÖ Local | ‚ö†Ô∏è Limited | ~0.30 | ‚ö° Fast |
| **Cloud RAG (GPT-4)** | ‚ùå Cloud | ‚úÖ Good | ~0.55 | üêå Slow |
| **Repo-JEPA** | ‚úÖ **Local** | ‚úÖ **Native** | ‚úÖ **0.905** | ‚ö° **Fast** |

---

## üî¨ Research Attribution

This work builds on:

1. **JEPA** - LeCun et al. (Meta AI, 2022)  
   *"A Path Towards Autonomous Machine Intelligence"*

2. **VICReg** - Bardes, Ponce, LeCun (ICLR 2022)  
   [ArXiv:2105.04906](https://arxiv.org/abs/2105.04906)

3. **CodeBERT** - Microsoft Research (EMNLP 2020)  
   [ArXiv:2002.08155](https://arxiv.org/abs/2002.08155)

**Novel Contribution**: First application of JEPA+VICReg to code-to-query semantic search.

---

## üí° Use Cases

- **Onboarding**: New developers search "where do we handle authentication?" without knowing internal naming conventions
- **Code Review**: Find all functions related to "data validation" semantically
- **Refactoring**: Locate similar logic across the codebase before consolidating
- **Documentation**: Generate function summaries by reverse-querying embeddings

---

## üõ†Ô∏è Advanced Usage

### Custom Training

```bash
# Train on your own dataset
python -m src.train \
  --dataset your_data.jsonl \
  --epochs 10 \
  --batch-size 64 \
  --checkpoint-dir ./custom_model
```

### Export to ONNX

```python
import torch
from transformers import AutoModel

model = AutoModel.from_pretrained("uddeshya-k/RepoJepa", trust_remote_code=True)
dummy_input = {"input_ids": torch.zeros(1, 512, dtype=torch.long)}
torch.onnx.export(model, dummy_input, "repojepa.onnx")
```

---

## ü§ù Contributing

We welcome:
- üêõ Bug reports on [GitHub Issues](https://github.com/uddeshya-23/Repo-JEPA/issues)
- üí° Feature requests (multi-language support, IDE plugins)
- üî¨ Research collaborations

---

## üìú Citation

```bibtex
@software{repojepa2026,
  author = {Uddeshya Kumar},
  title = {Repo-JEPA: Semantic Code Search via Joint Embedding Predictive Architecture},
  year = {2026},
  url = {https://huggingface.co/uddeshya-k/RepoJepa},
  note = {Based on JEPA (LeCun et al.) and VICReg (Bardes et al.)}
}
```

---

## üôè Acknowledgments

- **Yann LeCun** - JEPA framework
- **Meta AI** - VICReg research
- **Microsoft Research** - CodeBERT, CodeSearchNet
- **NVIDIA** - H100 infrastructure

---

**Questions?** Open a [Discussion](https://huggingface.co/uddeshya-k/RepoJepa/discussions) or check the [GitHub repo](https://github.com/uddeshya-23/Repo-JEPA).

**‚≠ê Star on GitHub if this helped you!**
